
# ğŸ› ï¸ Day 12: Basic Feature Engineering

Welcome to **Day 12** of the **100 Days of Data Mastery** journey!
Today, we focus on **Basic Feature Engineering**, the essential step of transforming raw data into meaningful inputs for machine learning models.
Through practical techniques, weâ€™ll clean, transform, scale, encode, and create new features to improve model performance and interpretability.

> **Note**: We work with a processed sales dataset to demonstrate real-world feature engineering techniques.

---

## ğŸ¯ Objectives

* Perform **data cleaning**:

  * Handle missing values using different imputation strategies.
  * Detect and treat outliers when necessary.
* Apply **feature transformation**:

  * Normalize or standardize numerical variables.
  * Encode categorical variables using one-hot encoding.
* Create **derived features**:

  * Combine or transform existing variables into new, more informative features.
* Perform **feature selection**:

  * Use statistical methods to retain only the most relevant variables.
* Visualize feature correlations to guide selection and transformation.

---

## ğŸ§° Tools & Libraries

* Python 3.9+
* numpy
* pandas
* matplotlib
* seaborn
* scikit-learn

Install all dependencies with:

```bash
pip install -r requirements.txt
```

---

## ğŸ“ Folder Structure

```
Day_12_Basic_Feature_Engineering/
â”‚
â”œâ”€â”€ README.md                        # This file: project overview and instructions
â”œâ”€â”€ basic_feature_engineering.ipynb  # Main notebook with feature engineering steps
â”œâ”€â”€ feature_engineering_utils.py     # Utility functions for preprocessing & feature creation
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed_sales_data.csv     # Example dataset for feature engineering
â””â”€â”€ outputs/
    â””â”€â”€ correlation_matrix.png       # Visualization of feature correlations
```

---

## ğŸ” Key Concepts Covered

* **Data Cleaning** â€” handling missing values, outliers, and inconsistent data.
* **Scaling & Normalization** â€” adjusting numerical data for better model performance.
* **Encoding Categorical Variables** â€” transforming categories into numerical form.
* **Feature Creation** â€” engineering new variables from existing data.
* **Feature Selection** â€” keeping only the most informative variables.
* **Correlation Analysis** â€” understanding relationships between features.

---

## ğŸ“Š Example Outputs

* Correlation heatmap of numerical features.
* Before/after visualizations of scaling and normalization.
* Encoded dataset ready for modeling.
* New derived features improving dataset richness.

---

## ğŸš€ Whatâ€™s Next?

**Day 13: Project â€” EDA + Visualization**
Weâ€™ll perform a full **Exploratory Data Analysis** on a real-world dataset, combining statistics and visualization techniques to uncover insights before modeling.

From transforming features to exploring data relationships â€” weâ€™re building a strong foundation for predictive modeling.

---